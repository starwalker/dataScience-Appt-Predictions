```{python imports, echo=False,results = 'hidden'}
import sys
# BIDev's Path
# sys.path.insert(0, 'C:\\Users\\bidev221\\PycharmProjects\\MUHA_Cost_Contribution')
# Austin's path
#sys.path.insert(0, 'C:/PyProjects/MUHA_Cost_Contribution')
#sys.path.insert(0, 'C:/pyCharm/Appointment_Predictions')

import pickle
import xgboost as xgb
import pandas as pd
import numpy as np
import shap
import gc
import random
from TextMiningMachine.splitting import TrainTestDateSplitter
from sklearn.model_selection import train_test_split
from TextMiningMachine.plot_methods import plot_regress_corr, plot_shap,plot_shap_univar,plot_cutoff_accuracy,plot_shap_multivar,plot_classification_metrics
import matplotlib.pyplot as plt
import sklearn
from scipy import sparse


# set target col
target_col = 'NonCompletedAppointments'
# load raw data
data = pd.read_pickle('data/raw_data_all.p')
target_vals = [1 if data.loc[i,'CompletedAppointments']==0 else 0 for i in range(data.shape[0])]
del data
# load transform
with open('models/text_cat_transformer_patientlevel_all.p', 'rb') as f:
    trans = pickle.load(f)

model_name = 'models/appointments_model_patientlevel_all.p'
# load the model
with open(model_name, 'rb') as f:
    model = pickle.load(f)



gc.collect()
# split the data

# load transformed data
with open('data/features_patientlevel.p', 'rb') as f:
    features = pickle.load(f)
features = sparse.csr_matrix(features)

# transform data into features
# features = trans.transform(data)

import random
from scipy import sparse
random.seed(2018)



X_train, X_test, y_train, y_test = train_test_split(features, target_vals, test_size=.6, random_state=0)
# split the data
X_test, X_holdout, y_test, y_holdout = train_test_split(X_test, y_test, test_size=.5, random_state=0)


# format the training and test sets
train = xgb.DMatrix(X_train, label=y_train, feature_names=trans.feature_names_clean)
test = xgb.DMatrix(X_test, label=y_test, feature_names=trans.feature_names_clean)

preds_test = model.predict(test)
preds_train = model.predict(train)
y_train=train.get_label()
y_test = test.get_label()

from sklearn.metrics import roc_curve, auc
preds_test = model.predict(test)
fpr, tpr, _ = roc_curve(y_test, preds_test)
preds_train = model.predict(train)
fpr_t, tpr_t, _ = roc_curve(y_train, preds_train)


import shap
import gc
gc.enable()
import random

## generate samples for sensitivity plots( this needs to go before the importance_frame code )
samp_size=10000
rand_inds = np.sort(random.sample(range(features.shape[0]),samp_size))
samp_data=features[rand_inds,:]
shap_vals = model.predict(xgb.DMatrix(samp_data,feature_names=model.feature_names),pred_contribs=True)
samp_df = pd.DataFrame(samp_data.todense())
samp_df.columns=model.feature_names



```
#### Objective to Predict Completed Appointments Among All Inpatients

The objective of model was predict the probability of an inpatient not completing their appointment.  The training data for the model was developed from the MUSC inpatient data pipeline.  Details are in the repository readme.  A combination of semi structured and numeric data from updated each night including diagnosis, problem lists, and therapeutic classes was used to create dependent variables.

#### Modeling Approach

The text data was run through a tokenizer to build a bag of words representation, with stop words removed.  The modeling technique used was gradient boosted tree via the xgboost package.  The data was split by putting 75% of the Hars into training and 25% to the test set.  The models were build using two optimizers, to maximize AUC and minimize missclassification log loss.  L1 and L2 regularization grid search was used to optimize the model

#### Variables Considered

The following variables were used to build the model.  The text columns are semi-structured and put into a bag of words model.  Catagorical columns were one hot encoded.  All numeric variables were zero imputed when missing.

```{python, variables, echo=False, results = 'tex'}
# training variables
print(trans.col_dict)
# final number of features
print('number of features : ', train.num_col() )
```

#### Holdout Set

```{python, echo=False, results = 'tex'}
print('testing rows : ', test.num_row())
print('training rows : ', train.num_row())
```

#### AUC of the ROC Curve
In a ROC curve, the true positive rate (Sensitivity) is plotted in function of the false positive rate (Specificity) for different cut-off values. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold.

```{python, AUC_Plot, echo=False}
# Calculate the AUC
roc_auc = auc(fpr, tpr)
roc_auc_t = auc(fpr_t, tpr_t)
#print('ROC AUC: %0.2f' % roc_auc)
plt.figure()
plt.plot(fpr, tpr, label='Test AUC  %0.2f' % roc_auc)
plt.plot(fpr_t, tpr_t, label='Train AUC %0.2f' % roc_auc_t)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()
```

#### Sensitvity and Specificity Curves
Sensitivity (true positive rate) is the proportion of cases where the model predicts positive given that the target variable is present. Specificity (true negative rate) is the proportion of cases where the model predicts negative given that the target variable is absent.

```{python,Sen_Spec_Curve,echo=False, evaluate=True,results = 'hidden'}
import pylab as pl
fpr, tpr, thresholds = roc_curve(y_test, preds_test)
roc_auc = auc(fpr, tpr)
#print("Area under the ROC curve : %f" % roc_auc)
i = np.arange(len(tpr)) # index for df
roc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i), '1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i), 'thresholds' : pd.Series(thresholds, index = i)})

roc.iloc[(roc.tf-0).abs().argsort()[:1]]

# Plot tpr vs 1-fpr
fig, ax = pl.subplots()
pl.plot(roc['tpr'], color='orange', label='sensitivity')
pl.plot(roc['1-fpr'], color = 'blue', label='specifity')
pl.xlabel('Cut Off')
pl.ylabel('Sensitiviy | Specificity')
pl.title('Sensitivity, Specificity vs Cut Off')
ax.set_xticklabels([])
plt.legend()
```
##### Predicted Probability Distribution
The distribution of model prediction is shown below. The red line represents the proportion of cases where the target variable is present.

```python Density Plots, echo = False
import seaborn as sns
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(4,4))
sns.distplot( preds_test, hist=False)
plt.axvline(sum(y_test)/len(y_test), color='r')
plt.title("Distribution of Predicted Probablities", loc='right')
plt.xlabel('Probablity')
plt.ylabel('Density')
plt.show()


```

#### Accuracy vs. Cutoff Plot
This plot displays the trade-off between cutoff value and accuracy. Optimal cutoff here is found by maximizing accuracy of model. This is not ideal with problems that have class imbalance; however, optimal cutoffs can be chosen when intervention costs are known.

```{python, accuracy_vs_cutoff, echo = False}
cut = plot_cutoff_accuracy(y_train, y_test, preds_train, preds_test)
```


#### Model Metrics using Using Above Cutoff
Sensitivity (true positive rate) is the proportion of cases where the model predicts positive when the target variable is present. Specificity (true negative rate) is the proportion of cases where the model predicts negative when the target variable is absent. Positive predictive value is the proportion of cases where the target variable is present when the model predicts positive. Negative predictive value is the proportion of cases where the target variable is absent when the model predicts negative.

```{python, fig = True, width=850, name='model_metrics', echo = False,evaluate=True}
plot_classification_metrics(y_train, y_test, preds_train, preds_test, cut)
```

#### Classification Report

```{python, classification_report, echo = False}
from sklearn.metrics import classification_report
print('using cut off : ' , cut)
y_test_pred_labels = [ 1 if p > cut else 0 for p in preds_test]
print(classification_report(y_test, y_test_pred_labels, target_names=['0','1']))
```




#### Variable Importance
The variable importance plots display the most important features by their information gain, which is a measurement that sums up how much 'information' a feature gives about the target variable. Information gain measures the reduction in entropy, or uncertainty, over each of the times that the given feature is split on.

```{python, variable_importance, width=850, echo=False,results = 'hidden'}
importances = model.get_score(importance_type='gain')
importance_frame = pd.DataFrame({'Importance': list(importances.values()), 'Feature': list(importances.keys())})
importance_frame.sort_values(by = 'Importance', inplace=True)
importance_frame = importance_frame.tail(20)
importance_frame.plot(x ='Feature', y='Importance' ,kind='barh',legend=False )
```



##### SHAP Univariate Plots
SHapley Additive exPlanations(SHAP) is an approach to explain the output of machine learning models. SHAP assigns a value to each feature for each prediction (i.e. feature attribution); the higher the value, the larger the featureâ€™s attribution to the specific prediction. In cases of classification, a positive SHAP value indicates that a factor increases the value of the model's prediction(risk), whereas a negative SHAP value indicates that a factor decreases the value of the model's prediction. The sum of SHAP values over all features will approximately equal the model prediction for each observation. In the following plots blue points signify negative SHAP values, red points have positive SHAP values, and yellow points are values for which the feature attributes little to predicted value.


```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}

#### plot the shap univariate plot for a given column
plt.figure()
plot_shap_univar('CompletedAppointmentPercentageLast12', shap_vals, samp_df,feature_names = samp_df.columns,logged_col=False)

```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}
#### plot the shap univariate plot for a given column
plt.figure()
plot_shap_univar('NoShowAppointmentsLast12', shap_vals, samp_df,feature_names = samp_df.columns,logged_col=False)

```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}

#### plot the shap univariate plot for a given column
plt.figure()
plot_shap_univar('PBServiceUnitsLast12', shap_vals, samp_df,feature_names = samp_df.columns,logged_col=False)

```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}

#### plot the shap univariate plot for a given column
plt.figure()
plot_shap_univar('PatientAge', shap_vals, samp_df,feature_names = samp_df.columns,logged_col=False)

```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}

#### plot the shap univariate plot for a given column
plt.figure()
plot_shap_univar('PBChargeAmountLast12', shap_vals, samp_df,feature_names = samp_df.columns,logged_col=False)

```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}

#### plot the shap univariate plot for a given column
plt.figure()
plot_shap_univar('PatientCancelPercentageLast12', shap_vals, samp_df,feature_names = samp_df.columns,logged_col=False)

```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}

#### plot the shap univariate plot for a given column
plt.figure()
plot_shap_univar('PatientCancelledWithin24HoursAppointmentsLast12', shap_vals, samp_df,feature_names = samp_df.columns,logged_col=False)

```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}

#### plot the shap univariate plot for a given column
plt.figure()
plot_shap_univar('AppointmentsLast12', shap_vals, samp_df,feature_names = samp_df.columns,logged_col=False)

```
```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}

#### plot the shap univariate plot for a given column
plt.figure()
plot_shap_univar('CompletedNurseOrAncillaryAppointmentsLast12', shap_vals, samp_df,feature_names = samp_df.columns,logged_col=False)

```

```{python, fig = True, width=400, name = 'SHAP',echo=False, evaluate=True}

#### plot the shap univariate plot for a given column
plt.figure()
plot_shap_univar('ProviderCancelledAppointmentsLast12', shap_vals, samp_df,feature_names = samp_df.columns,logged_col=False)

```










##### SHAP Summary Plot Over Categorical/Text Columns
The following plot display the effect of the top text/category values on the Model's predictions. Red data points represent when the text/column feature is observed in a patient's records, whereas the blue represents when the feature is not observed.
```{python, fig = True,width=850, name = 'SHAP Cat/Text Summary',echo=False, evaluate=True,results = 'hidden'}

# this section plots the SHAP summary plot using only categorical&text features from the important features above

# pluck out the indices associated with each of the top features(from importance frame) into a list
feature_inds = [np.where([feature == model.feature_names[i] for i in range(len(model.feature_names))])[0][0] for feature in importance_frame.Feature]+[1]
# go through and find which of the top features are a text_col or cat_col.
# this is achieved by eliminating all numeric(non text/cat) columns
non_encoded_types = [col for col in list(trans.col_dict.keys()) if col not in ('text_cols','cat_cols')]
# keeps track of which of the top features aren't encoded
non_encoded_feats=[]
for feature in importance_frame.Feature:
    for type in non_encoded_types:
        if feature in trans.col_dict.get(type):
            non_encoded_feats.append(feature)

# extract encoded_feats by elimnating the non-encoded features from the important features
encoded_feats = [col for col in importance_frame.Feature if col not in non_encoded_feats]
plt.figure()
#plt.subplots_adjust(left=0, bottom=None, right=1, top=None, wspace=None, hspace=None)
encoded_feat_inds = [np.where([feature == model.feature_names[i] for i in range(len(model.feature_names))])[0][0] for feature in encoded_feats]+[1]
shap.summary_plot(shap_vals[:,encoded_feat_inds], samp_df.iloc[:,encoded_feat_inds], show=False)#,auto_size_plot=False
plt.subplots_adjust(left=.3, bottom=None, right=.95, top=None, wspace=None, hspace=None)
plt.show
```
